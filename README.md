# ğŸ Snake-IA
**Agente de BÃºsqueda para Resolver Snake**

**Integrantes:** JosÃ© Briones, NicolÃ¡s Lintz, Ignacio Barrientos, Felipe Aguayo  
**Carrera:** IngenierÃ­a en InformÃ¡tica  
**SecciÃ³n:** TI2082/D-IEI-N8-P1-C2/D Valdivia IEI  
**Asignatura:** Aplicaciones de Inteligencia Artificial  
**Fecha:** 24/09/2025  
**Docente:** Vedran Tomicic Cantizano  
 
Este repositorio contiene una implementaciÃ³n del clÃ¡sico juego **Snake (10x10, mÃ¡x. 35 manzanas)** junto con dos agentes:

- **Agente experto (A\*)**: utiliza bÃºsqueda informada con heurÃ­stica Manhattan para encontrar caminos Ã³ptimos hacia la manzana.
- **Agente modelo (Imitation Learning)**: una red neuronal entrenada a partir de la experiencia del agente A\* para imitar su comportamiento.
## Objetivo del proyecto
- Implementar un **algoritmo de bÃºsqueda** (informado o no) para resolver Snake.
- Evaluar la capacidad del agente para:
  - Evitar colisiones.
  - Recolectar manzanas eficientemente.
- Medir y reportar el **tiempo de resoluciÃ³n** de cada ejecuciÃ³n.
- Documentar el cÃ³digo y explicar la elecciÃ³n del algoritmo.

## ğŸ“‚ Estructura del repositorio
snake-ia-main/
â”‚â”€â”€ agents/ # Agentes del juego
â”‚ â”œâ”€â”€ astar_agent.py # Agente experto con A*
â”‚ â””â”€â”€ model_agent.py # Red neuronal y funciones de Imitation Learning
â”‚
â”‚â”€â”€ env/
â”‚ â””â”€â”€ snake_env.py # Entorno del juego Snake (tablero, reglas, colisiones)
â”‚
â”‚â”€â”€ logs/
â”‚ â””â”€â”€ metrics.csv # Registro de resultados (manzanas, pasos, tiempo, motivo)
â”‚
â”‚â”€â”€ models/
â”‚ â””â”€â”€ snake_model.pt # Modelo entrenado (PyTorch)
â”‚
â”‚â”€â”€ utils/
â”‚ â”œâ”€â”€ visual.py # VisualizaciÃ³n con pygame (grÃ¡ficos, HUD, sonidos)
â”‚ â”œâ”€â”€ loco_manzana.png # Imagen opcional de la manzana
â”‚ â”œâ”€â”€ rene_hola.mp3 # Sonido de inicio (opcional)
â”‚ â””â”€â”€ rene_miau.mp3 # Sonido de game over (opcional)
â”‚
â”‚â”€â”€ snake_ia.py # Script principal de entrenamiento y evaluaciÃ³n
â”‚â”€â”€ requirements.txt # Dependencias necesarias
----------------------------------------------------------------------
ğŸArchivo principal
1) snake_ia.py
Rol: Script principal que coordina todo el proyecto.
QuÃ© hace: Inicializa el entorno Snake (SnakeEnv).
Define y entrena (o carga) el modelo neuronal.
Ejecuta y evalÃºa al agente experto A* y al agente modelo.
Registra mÃ©tricas en logs/metrics.csv.
Permite visualizaciÃ³n grÃ¡fica con pygame o ejecuciÃ³n en modo texto.
Uso: se ejecuta desde la terminal con:
python snake_ia.py (modo texto).
python snake_ia.py --vis (modo grÃ¡fico).

Carpeta agents/
Contiene los agentes que juegan Snake (es decir, las estrategias para mover la serpiente).
2) astar_agent.py
Implementa el agente A*.
Incluye: manhattan: funciÃ³n heurÃ­stica (distancia Manhattan).
astar_path: encuentra la ruta mÃ¡s corta hacia la manzana evitando colisiones.
path_to_direction: traduce una ruta en un movimiento (arriba, abajo, izquierda, derecha).
expert_agent: agente experto que toma decisiones usando A*.
Rol: sirve como "maestro" para entrenar al modelo y como baseline de comparaciÃ³n.

3)model_agent.py
Implementa el agente modelo basado en aprendizaje por imitaciÃ³n (Imitation Learning).
Incluye:
PolicyNet: red neuronal en PyTorch.
collect_expert_data: genera dataset de jugadas del agente A*.
train_imitation: entrena el modelo con esos datos.
model_agent_factory: convierte el modelo en un agente que puede interactuar con SnakeEnv.
Rol: agente "aprendiz" que imita al experto y generaliza jugadas.

ğŸŒ Carpeta env/
Contiene la definiciÃ³n del entorno del juego Snake.

4) snake_env.py
Implementa la clase SnakeEnv.
Funcionalidades:
reset: reinicia el tablero, coloca la serpiente y la primera manzana.
step: ejecuta un movimiento, actualiza el estado y calcula recompensas.
available_actions: lista de movimientos vÃ¡lidos desde el estado actual.
run_episode: corre un episodio completo de Snake con un agente dado.
render_text: muestra el tablero en modo texto.
Rol: define las reglas del juego (colisiones, crecimiento, victoria/derrota).

ğŸ“Š Carpeta logs/

Contiene archivos de registro de mÃ©tricas.
5) metrics.csv
CSV con resultados de las ejecuciones:
NÃºmero de episodio.
Agente utilizado (A* o Modelo).
Manzanas recolectadas.
Pasos realizados.
Tiempo del episodio.
Motivo de finalizaciÃ³n (wall, self, max_steps, etc.).
Rol: permite analizar y comparar el desempeÃ±o de los agentes.

ğŸ§  Carpeta models/

Contiene el modelo entrenado en PyTorch.
6) snake_model.pt
Archivo binario que guarda los pesos entrenados de la red neuronal.
Permite cargar el modelo sin necesidad de reentrenarlo en cada ejecuciÃ³n.
Rol: persistencia del agente modelo.

ğŸ¨ Carpeta utils/

Contiene herramientas de apoyo (visualizaciÃ³n y recursos).
7) visual.py
Maneja la visualizaciÃ³n del juego con pygame.
Funcionalidades:
Dibuja tablero, serpiente y manzana.
Muestra HUD con manzanas, agente y tiempo.
Muestra mensajes de victoria o derrota.
Reproduce sonidos opcionales (rene_hola.mp3, rene_miau.mp3).
Usa imagen personalizada para la manzana (loco_manzana.png).
Rol: experiencia grÃ¡fica para visualizar y presentar el juego.
Recursos adicionales:
loco_manzana.png: imagen para representar la manzana.
rene_hola.mp3: sonido que se reproduce al iniciar.
rene_miau.mp3: sonido de game over.

ğŸ“¦ Otros archivos
8) requirements.txt
Lista de dependencias necesarias para correr el proyecto:
asgiref==3.9.1
Django==5.2.5
filelock==3.19.1
fsspec==2025.9.0
Jinja2==3.1.6
MarkupSafe==3.0.2
mpmath==1.3.0
networkx==3.5
numpy==2.3.3
pygame==2.6.1
setuptools==80.9.0
sqlparse==0.5.3
sympy==1.14.0
torch==2.8.0
typing_extensions==4.15.0
tzdata==2025.2
Rol: asegura que cualquier usuario pueda instalar el entorno correcto.
Para instalar todas las dependencias necesarias, es necesario ejecutar el siguiente comando: pip install -r requirements.txt

MÃ©tricas y evaluaciÃ³n:

Cada ejecuciÃ³n se registra en logs/metrics.csv con:

Episodio: nÃºmero de corrida.

Agente: "A*" o "Modelo".

Manzanas: recolectadas antes de terminar.

Pasos: movimientos realizados.

Tiempo: duraciÃ³n en segundos.

Motivo: por quÃ© terminÃ³ (ej: wall, self, max_steps, done).

Ejemplo: Episodio,Agente,Manzanas,Pasos,Tiempo,Motivo
1,A*,35,311,31.16,done
1,Modelo,1,1000,100.29,max_steps *Se puede ver en metrics.csv*

----------------------------------------------------------------------
## âš™ï¸ InstalaciÃ³n
1. Clonar este repositorio o descargar el `.zip`.
2. Abrir el proyecto en **Visual Studio Code**.
3. Crear un entorno virtual (recomendado):
   ```powershell
   python -m venv .venv
   .\.venv\Scripts\activate
'''En caso de aparecer error ejecutar en PowerShell como admin: Set-ExecutionPolicy RemoteSigned
4. Instalar dependencias: pip install -r requirements.txt
5. Ejecutar juego: python snake_ia.py --vis
----------------------------------
Durante el juego verÃ¡s:
Presiona SPACE para comenzar.
ğŸŸ© Serpiente (cabeza mÃ¡s intensa, cuerpo degradado).

ğŸ Manzana (imagen).

HUD con:
Manzanas comidas.
Nombre del agente (A* o Modelo).
Tiempo del episodio.
-----------------------------

Algoritmos utilizados
1. Agente Experto (A*)
Tipo: BÃºsqueda informada.
HeurÃ­stica: Distancia Manhattan (|x1 - x2| + |y1 - y2|).

Ventajas:
Encuentra caminos cortos hacia la manzana.
Evita chocar contra la pared o el cuerpo.

1.- Ã“ptimo en trayectorias â†’ siempre encuentra la ruta mÃ¡s corta desde la cabeza de la serpiente hasta la manzana, evitando rodeos innecesarios.
2.- Evita muertes innecesarias â†’ a diferencia de un agente greedy o aleatorio, considera colisiones con cuerpo y paredes, reduciendo la probabilidad de perder de forma tonta.
3.- Sirve como profesor para el modelo â†’ genera un dataset estable y confiable para entrenar la red neuronal. Si el experto fuese inconsistente, el modelo aprenderÃ­a mal.
4.- Simplicidad de implementaciÃ³n â†’ balancea eficiencia, calidad de las decisiones y facilidad de programar/debuggear en comparaciÃ³n con mÃ©todos mÃ¡s complejos como MCTS o Q-learning.
5.- Cumple con los objetivos del proyecto â†’ el enunciado pedÃ­a un algoritmo de bÃºsqueda y A* es el estÃ¡ndar clÃ¡sico en este tipo de problemas.

Limitaciones: En escenarios donde la serpiente ocupa gran parte del tablero, puede quedarse sin soluciones viables y terminar atrapada.

2. Agente Modelo (Imitation Learning)

Tipo: Red neuronal feed-forward (PyTorch):
Arquitectura: Input: tablero (2 canales: serpiente y manzana).
Capas ocultas: 256 â†’ 128 â†’ 4 salidas (acciones).

Entrenamiento: Dataset generado con el agente A*.
FunciÃ³n de pÃ©rdida: CrossEntropy.
Optimizador: Adam.

Ventajas:

Generaliza jugadas sin necesidad de calcular el A* en cada paso.
MÃ¡s rÃ¡pido en ejecuciÃ³n.

Â¿Por quÃ© un modelo de Imitation Learning?
1.- Aprende de un experto confiable â†’ A* actÃºa como maestro, permitiendo que el modelo generalice buenas jugadas.
2.- Mayor velocidad de ejecuciÃ³n â†’ una vez entrenado, el modelo decide acciones sin necesidad de recalcular una bÃºsqueda A* en cada paso.
3.- Capacidad de generalizar â†’ puede encontrar jugadas razonables en estados no vistos durante el entrenamiento.
4.- IntegraciÃ³n con tÃ©cnicas modernas de IA â†’ conecta la parte de bÃºsqueda clÃ¡sica con mÃ©todos de aprendizaje automÃ¡tico.

Limitaciones: No siempre garantiza la mejor ruta y su rendimiento depende directamente de la calidad y cantidad de los datos de entrenamiento recolectados del Dataset (agente A*).

------------------------------------------------------------------------------
